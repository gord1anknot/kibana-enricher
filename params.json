{"name":"Kibana Enricher","tagline":"denormalize Logstash / Kibana events with additional context asynchronously as a batch job","body":"# Logstash Enrichment\r\n\r\nThis project enriches Logstash formatted Elasticsearch documents with additional context asynchronously as a batch job, using the [Elasticsearch update document API](https://www.elastic.co/guide/en/elasticsearch/reference/1.4/docs-update.html).\r\n\r\nBecause denormalized data is better for Kibana 3 searches.\r\n\r\n## Use Case\r\n\r\nThis project helps prevent the following situation:\r\n\r\nYou have some web service that takes HTTP requests from a client and creates logs that looks like this:\r\n\r\n`{ \"type\": \"REPORT-REQUEST\", \"report.type\": \"Cat Videos\", \"id\": \"DEADBEEF-1234-XC15-8341-BEEF9D4BB6D2\", \"source.ip\": \"10.210.100.90\"}`\r\n\r\n`{ \"type\": \"REPORT-REQUEST\", \"report.type\": \"Tweets\", \"id\": \"AFFCEABA-4950-4C15-8341-8E959D4BB6D2\",  \"source.ip\": \"10.210.100.93\"}`\r\n\r\nYou have a different web service that asynchronously creates the requests report. It generates logs that look like this:\r\n\r\n`{ \"type\": \"REPORT-CREATED\", \"report.type\": \"Tweets\", \"id\": \"AFFCEABA-4950-4C15-8341-8E959D4BB6D2\", \"time.taken\": \"992000\", \"size\": \"390k\"}`\r\n\r\n`{ \"type\": \"REPORT-CREATED\", \"report.type\": \"Cat Videos\", \"id\": \"DEADBEEF-1234-XC15-8341-BEEF9D4BB6D2\", \"time.taken\": \"25393020\", \"size\": \"204930k\"}`\r\n\r\nYour pager goes off at 3:00 AM - Zabbix has told you that long transaction times are causing a service to crash. Thanks to a very\r\nsimple search inside Kibana, you already know several of the `report.type`s that appear in the problem requests, but\r\nyou still need to figure out which single `source.ip`, if any, is responsible for the huge `time.taken` values that are crashing\r\nyour server so that you filter that source out.\r\n\r\nKibana 3 has a `top N` query, so you can search for the `top N` `source.ip` addresses on all documents with no issue.\r\nHowever, the documents with `type:REPORT-CREATED` are the only ones with the value you care about - `time.taken`.\r\n`REPORT-CREATED` documents will not show up in any query using `source.ip` in a filter or facet, and a Kibana 3 `top N`\r\nquery isn't helpful either. You need the aggregation query API to figure out which `source.ip` is submitting report requests\r\nthat generate the huge reports. Unfortunately you don't know anything about Elasticsearch, and the guy that does is on vacation... you only know how to use Kibana 3.\r\n\r\nInstead of that mess, this project can run as a cron job all the time like this:\r\n\r\n```\r\n./kibana-enricher \\\r\n    -idField=\"id\" \\\r\n    -idValue=\"AFFCEABA-4950-4C15-8341-8E959D4BB6D2\"\r\n    -json='{\"report.type\":\"Cat Videos\", \"source.ip\": \"10.210.100.93\"}'\r\n```\r\n\r\nNow all your `type:REPORT-CREATED` documents have a `\"source.ip\"` field. Now, no power in the 'verse can stop you. Barring hardware failure of course but let's not jinx it.\r\n\r\n## Usage\r\n\r\n\r\n\r\n## Build and Deployment\r\n\r\nInstall dependencies with `go get`:\r\n\r\n```\r\ngo get github.com/mattbaird/elastigo\r\n```\r\n\r\nAlternatively, if you have many go-lang projects you can use vendorized dependencies courtesy [`gvp`](https://github.com/pote/gvp) and [`gpm`](https://github.com/pote/gpm#go-package-manager-):\r\n\r\nOSX Installation of `gpm` and `gvp`:\r\n\r\n```\r\nbrew update && brew install gpm gvp\r\n```\r\n\r\nUsage:\r\n\r\n```\r\nsource gvp\r\ngpm install\r\n```\r\n\r\nAfter that, build the project:\r\n\r\n```\r\ngo build .\r\n```\r\n\r\nDeploy the binary `kibana-enricher` as you see fit. When using Ubuntu on AWS, I will do something like this:\r\n\r\n```\r\nscp kibana-enricher ubuntu@my-deployment-target:/home/ubuntu/\r\nssh ubuntu@my-deployment-target -- sudo mv /home/ubuntu/kibana-enricher /usr/local/bin/\r\n```\r\n\r\nRun the binary on a regular schedule using `cron` or similar.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}